{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import cv2\n",
    "import string\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warning in the output\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6094782701062088614,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 5588946784\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 10093268770664928384\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check GPU/CPU\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_list:   'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "# total number of our output classes: len(char_list)\n",
    "char_list = string.ascii_letters+string.digits\n",
    " \n",
    "def encode_to_labels(txt):\n",
    "    # encoding each output word into digits\n",
    "    dig_lst = []\n",
    "    for index, char in enumerate(txt):\n",
    "        try:\n",
    "            dig_lst.append(char_list.index(char))\n",
    "        except:\n",
    "            print(char)\n",
    "        \n",
    "    return dig_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/home/sunt/Downloads/mnt/ramdisk/max/90kDICT32px'\n",
    " \n",
    "# lists for training dataset\n",
    "training_img = []\n",
    "training_txt = []\n",
    "train_input_length = []\n",
    "train_label_length = []\n",
    "orig_txt = []\n",
    " \n",
    "#lists for validation dataset\n",
    "valid_img = []\n",
    "valid_txt = []\n",
    "valid_input_length = []\n",
    "valid_label_length = []\n",
    "valid_orig_txt = []\n",
    " \n",
    "max_label_len = 0\n",
    " \n",
    "i =1 \n",
    "flag = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirnames, filenames in os.walk(path):\n",
    " \n",
    "    for f_name in fnmatch.filter(filenames, '*.jpg'):\n",
    "        # read input image and convert into gray scale image\n",
    "        img = cv2.cvtColor(cv2.imread(os.path.join(root, f_name)), cv2.COLOR_BGR2GRAY)   \n",
    " \n",
    "        # convert each image of shape (32, 128, 1)\n",
    "        w, h = img.shape\n",
    "        if h > 128 or w > 32:\n",
    "            continue\n",
    "        if w < 32:\n",
    "            add_zeros = np.ones((32-w, h))*255\n",
    "            img = np.concatenate((img, add_zeros))\n",
    " \n",
    "        if h < 128:\n",
    "            add_zeros = np.ones((32, 128-h))*255\n",
    "            img = np.concatenate((img, add_zeros), axis=1)\n",
    "        img = np.expand_dims(img , axis = 2)\n",
    "        \n",
    "        # Normalize each image\n",
    "        img = img/255.\n",
    "        \n",
    "        # get the text from the image\n",
    "        txt = f_name.split('_')[1]\n",
    "        \n",
    "        # compute maximum length of the text\n",
    "        if len(txt) > max_label_len:\n",
    "            max_label_len = len(txt)\n",
    "            \n",
    "           \n",
    "        # split the 150000 data into validation and training dataset as 10% and 90% respectively\n",
    "        if i%10 == 0:     \n",
    "            valid_orig_txt.append(txt)   \n",
    "            valid_label_length.append(len(txt))\n",
    "            valid_input_length.append(31)\n",
    "            valid_img.append(img)\n",
    "            valid_txt.append(encode_to_labels(txt))\n",
    "        else:\n",
    "            orig_txt.append(txt)   \n",
    "            train_label_length.append(len(txt))\n",
    "            train_input_length.append(31)\n",
    "            training_img.append(img)\n",
    "            training_txt.append(encode_to_labels(txt)) \n",
    "        \n",
    "        # break the loop if total data is 150000\n",
    "        if i == 15000:\n",
    "            flag = 1\n",
    "            break\n",
    "        i+=1\n",
    "    if flag == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad each output label to maximum text length\n",
    "\n",
    "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
    "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input with shape of height=32 and width=128 \n",
    "inputs = Input(shape=(32,128,1))\n",
    "\n",
    "# convolution layer with kernel size (3,3)\n",
    "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "# poolig layer with kernel size (2,2)\n",
    "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    "\n",
    "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    "\n",
    "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    "\n",
    "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
    "# poolig layer with kernel size (2,1)\n",
    "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
    "\n",
    "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
    "# Batch normalization layer\n",
    "batch_norm_5 = BatchNormalization()(conv_5)\n",
    "\n",
    "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "batch_norm_6 = BatchNormalization()(conv_6)\n",
    "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
    "\n",
    "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
    "\n",
    "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
    "\n",
    "# bidirectional LSTM layers with units=128\n",
    "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    "\n",
    "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "\n",
    "# model to be used at test time\n",
    "act_model = Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 128, 1)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 128, 64)       640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 32, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 32, 256)        295168    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 32, 256)        590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 32, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 32, 512)        1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4, 32, 512)        2048      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 32, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 32, 512)        2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 32, 512)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 1, 31, 512)        1049088   \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 31, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 31, 256)           656384    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 31, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 31, 63)            16191     \n",
      "=================================================================\n",
      "Total params: 6,619,711\n",
      "Trainable params: 6,617,663\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "crnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    " \n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
    "\n",
    "#model to be used at training time\n",
    "crnn_model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn_model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"best_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_img = np.array(training_img)\n",
    "train_input_length = np.array(train_input_length)\n",
    "train_label_length = np.array(train_label_length)\n",
    "\n",
    "valid_img = np.array(valid_img)\n",
    "valid_input_length = np.array(valid_input_length)\n",
    "valid_label_length = np.array(valid_label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 31s 56ms/step - loss: 30.0302 - val_loss: 27.1889\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 27.18888, saving model to best_model.hdf5\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 21s 50ms/step - loss: 27.1102 - val_loss: 27.6841\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 27.18888\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 21s 50ms/step - loss: 26.8504 - val_loss: 29.3525\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 27.18888\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 26.6582 - val_loss: 26.8355\n",
      "\n",
      "Epoch 00004: val_loss improved from 27.18888 to 26.83552, saving model to best_model.hdf5\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 22s 51ms/step - loss: 26.2419 - val_loss: 26.0609\n",
      "\n",
      "Epoch 00005: val_loss improved from 26.83552 to 26.06088, saving model to best_model.hdf5\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 25.5828 - val_loss: 25.8229\n",
      "\n",
      "Epoch 00006: val_loss improved from 26.06088 to 25.82286, saving model to best_model.hdf5\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 24.6577 - val_loss: 24.4280\n",
      "\n",
      "Epoch 00007: val_loss improved from 25.82286 to 24.42803, saving model to best_model.hdf5\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 23.7774 - val_loss: 23.2948\n",
      "\n",
      "Epoch 00008: val_loss improved from 24.42803 to 23.29477, saving model to best_model.hdf5\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 22.3487 - val_loss: 21.9169\n",
      "\n",
      "Epoch 00009: val_loss improved from 23.29477 to 21.91691, saving model to best_model.hdf5\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 22s 52ms/step - loss: 20.4303 - val_loss: 19.2366\n",
      "\n",
      "Epoch 00010: val_loss improved from 21.91691 to 19.23656, saving model to best_model.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f59807c2a00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "crnn_model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_text =   lacewing\n",
      "predicted text = hoing\n",
      "\n",
      "original_text =   whiners\n",
      "predicted text = Cilies\n",
      "\n",
      "original_text =   sightly\n",
      "predicted text = sedy\n",
      "\n",
      "original_text =   AMONG\n",
      "predicted text = Aas\n",
      "\n",
      "original_text =   Kubrick\n",
      "predicted text = bibad\n",
      "\n",
      "original_text =   issuers\n",
      "predicted text = Les\n",
      "\n",
      "original_text =   TWAT\n",
      "predicted text = TD\n",
      "\n",
      "original_text =   Folklore\n",
      "predicted text = Ca\n",
      "\n",
      "original_text =   HMO\n",
      "predicted text = TEd\n",
      "\n",
      "original_text =   PALLETS\n",
      "predicted text = TEES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the saved best model weights\n",
    "act_model.load_weights('best_model.hdf5')\n",
    " \n",
    "# predict outputs on validation images\n",
    "prediction = act_model.predict(valid_img[:10])\n",
    " \n",
    "# use CTC decoder\n",
    "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
    "                         greedy=True)[0][0])\n",
    " \n",
    "# see the results\n",
    "i = 0\n",
    "for x in out:\n",
    "    print(\"original_text =  \", valid_orig_txt[i])\n",
    "    print(\"predicted text = \", end = '')\n",
    "    for p in x:  \n",
    "        if int(p) != -1:\n",
    "            print(char_list[int(p)], end = '')       \n",
    "    print('\\n')\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
